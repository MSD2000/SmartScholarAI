# SmartScholar: ArXiv Paper Search (MiniLM)

SmartScholar is a lightweight semantic search interface for ArXiv papers. It allows users to:

1. **Search by exact paper title** (case-insensitive) and immediately retrieve that paper.
2. **Enter keywords or partial titles** to retrieve the top-K most semantically similar papers, powered by the all-MiniLM-L6-v2 SentenceTransformer model.

Each result is displayed as a â€œcardâ€ containing:
- **Title** (and semantic similarity score, if applicable);
- **Authors** (each author name is a hyperlink to their Google Scholar profile);
- **Abstract** (shown in full);
- **â€œRead via DOIâ€** and **â€œSearch on Google Scholarâ€** hyperlinks;
- An expandable **â€œShow additional infoâ€** section revealing metadata such as submitter, comments, journal reference, categories, license, update date, parsed authors, and main category;
- A help button that uses a locally deployed model (tinyllama) to explain the abstract in simple terms and give keyword examples;
- Text-to-speech (pyttsx3) for reading the help, as an accessibility option.

---

## ğŸ“ Repository Structure

```
SmartScholarAI/
â”œâ”€â”€ all-MiniLM-L6-v2/             # Optional folder if you store model locally
â”œâ”€â”€ app/
â”‚   â””â”€â”€ smart_scholar_ai_v3.py    # Main Streamlit application
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ arxiv_tokenized_balanced.csv   # Tokenized-balanced metadata CSV
â”‚   â””â”€â”€ arxiv_minilm_embeddings.npy    # Precomputed MiniLM embeddings (NÃ—384)
â”œâ”€â”€ notebooks/                    # Jupyter notebooks (optional)
â”œâ”€â”€ scripts/                      # Helper scripts (optional)
â”œâ”€â”€ venv/                         # Virtual environment directory (optional)
â”œâ”€â”€ LICENSE                       # Project license (e.g., MIT)
â”œâ”€â”€ README.md                     # This file
â””â”€â”€ requirements.txt              # Python dependencies
```

- **app/app_streamlit.py**  
  The Streamlit app that loads metadata + embeddings, handles exactâ€title and semantic keyword searches, and renders results.

- **data/arxiv_tokenized_balanced.csv**  
  The tokenizedâ€balanced subset of ArXiv papers. Columns include:
  ```
  title, abstract, submitter, authors, comments, journal-ref, doi, report-no,
  categories, license, update_date, authors_parsed, main_category, title_norm
  ```
  (The `title_norm` column is added on load to facilitate case-insensitive title matching.)

- **data/arxiv_minilm_embeddings.npy**  
  A memory-mapped NumPy array of shape (N_papers Ã— 384), containing all-MiniLM-L6-v2 embeddings for each row in `arxiv_tokenized_balanced.csv`.

- **requirements.txt**  
  A list of required Python packages:
  ```txt
  altair
  attrs
  blinker
  cachetools
  certifi
  charset-normalizer
  click
  colorama
  comtypes
  filelock
  fsspec
  gitdb
  GitPython
  huggingface-hub
  idna
  Jinja2
  joblib
  jsonschema
  jsonschema-specifications
  MarkupSafe
  mpmath
  narwhals
  networkx
  numpy
  packaging
  pandas
  pillow
  protobuf
  pyarrow
  pydeck
  pypiwin32
  python-dateutil
  pyttsx3
  pytz
  pywin32
  PyYAML
  referencing
  regex
  requests
  rpds-py
  safetensors
  scikit-learn
  scipy
  sentence-transformers
  six
  smmap
  streamlit
  sympy
  tenacity
  threadpoolctl
  tokenizers
  toml
  torch
  tornado
  tqdm
  transformers
  typing_extensions
  tzdata
  urllib3
  watchdog
  ```

- **LICENSE**  
  The project license (e.g., MIT) describing how the code can be used and distributed.

---

## ğŸ›  Dependencies

Make sure you have locally deployed Ollama with tinyllama.
Make sure you have locally downloaded inside the root project folder all-MiniLM-L6-v2 model.
Make sure you have **Python 3.10+** installed.

Install the required libraries with:

```bash
pip install -r requirements.txt
```

This will install:

- **Streamlit**: For building the interactive web interface.  
- **Pandas** & **NumPy**: For data loading and manipulation.  
- **scikit-learn**: For cosine similarity computations.  
- **sentence-transformers**: To load the all-MiniLM-L6-v2 model for embedding queries.

---

## ğŸš€ Running the App

1. **Clone the repository** (or download/unwrap the ZIP):

   ```bash
   git clone https://github.com/MSD2000/SmartScholarAI.git
   cd SmartScholarAI
   ```

2. **Verify data files**  
   Ensure the following files exist:

   ```
   SmartScholarAI/
   â”œâ”€â”€ app/
   â”‚   â””â”€â”€ app_streamlit.py
   â””â”€â”€ data/
       â”œâ”€â”€ arxiv_tokenized_balanced.csv
       â””â”€â”€ arxiv_minilm_embeddings.npy
   ```

   If you donâ€™t have these data files, please download them from your data source and place them under `data/` exactly as named above.

3. **Activate your virtual environment** (if using one):

   ```bash
   source venv/bin/activate    # macOS/Linux
   .\venv\Scripts\activate  # Windows
   ```

4. **Install dependencies** (if you havenâ€™t already):

   ```bash
   pip install -r requirements.txt
   ```

5. **Launch Streamlit**:

   ```bash
   streamlit run app/app_streamlit.py
   ```

6. **Open your browser**  
   Streamlit will start a local server (by default at `http://localhost:8501`). Navigate there to use the search interface.

---

## ğŸ” How It Works

1. **Metadata Loading**  
   - On startup, `app/app_streamlit.py` calls `pd.read_csv("data/arxiv_tokenized_balanced.csv", usecols=[â€¦])` to load only the needed columns into a DataFrame (`df_meta`).  
   - It then adds a normalized column `title_norm = title.lower().strip()` for quick exactâ€title matching.

2. **Embedding Loading (Memory-Mapped)**  
   - The precomputed NumPy embeddings are loaded using `np.load("data/arxiv_minilm_embeddings.npy", mmap_mode="r")`.  
   - Memory-mapping ensures the `.npy` file is not fully read into RAM at once.

3. **Exactâ€Title Match**  
   - When the user submits a query, the app checks if `query.lower()` equals any `df_meta["title_norm"]`.  
   - If an exact match is found, those paper(s) are displayed immediately (no semantic step).

4. **Semantic Search**  
   - If no exact match, the app loads the all-MiniLM-L6-v2 model once (cached by Streamlit).  
   - The query is encoded into a 384-dim vector.  
   - Cosine similarities are computed between that query vector and each row of the memory-mapped embeddings.  
   - The top-K highestâ€similarity papers are selected and displayed in descending order of similarity score.

5. **Result Rendering**  
   Each result card includes:  
   - **Title** (with a small â€œscoreâ€ badge if semantic search)  
   - **Authors** (split on commas/â€œandâ€ via a regex, then rendered inline with each author as a clickable link to their Google Scholar profile)  
   - **Full Abstract**  
   - **Links Row**:  
     - â€œğŸ”— Read via DOIâ€ (if the paper has a DOI)  
     - â€œğŸ” Search on Google Scholarâ€ (searches by title)  
   - **Expandable Section** (â€œShow additional infoâ€) revealing fields:  
     - `submitter`, `comments`, `journal-ref`, `report-no`, `categories`, `license`, `update_date`, `authors_parsed`, `main_category`.

---

## ğŸ™‹â€â™‚ï¸ Authors

- [Vasil Kozov](https://github.com/VasilKozov) (post-doc) | vkozov@uni-ruse.bg
- [Martin Dzhurov](https://github.com/MSD2000) (masters-degree student) | mdzhurov@uni-ruse.bg
- [Kristian Spasov](https://github.com/kristianinator) (masters-degree student) | kspasov@uni-ruse.bg

University of Ruse "Angel Kanchev" - Ruse, Bulgaria
